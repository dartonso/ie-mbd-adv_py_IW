{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Programming with Python: Bike Sharing Prediction\n",
    "**Group Assignment**  \n",
    "By Team O2-2 (B)  \n",
    "eddited by Duarte for Advanced Python Individual Assignment\n",
    "\n",
    "Disclaimer: I had to performe some heavy transformation on the original model submitted to evaluation. Mostly it had to do with cleaning noise and make everything more smooth for the purpose of this assignment. Performance was not much affected and I kept all the main sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Libraries](#1.-Libraries)  \n",
    "[2. Load dataset](#2.-Load-dataset)  \n",
    "[3. Dask basic config](#3.-Dask-basic-configuration)  \n",
    "[4. Data preparation](#4.-Data-preparation)  \n",
    "[5. Dask ML algos](#5.-Dask-Machine-Learning-algos)  \n",
    "> [5.1 Linear regression](#5.1-Linear-regression)  \n",
    "> [5.2 Random forest](#5.2-Random-forest)  \n",
    "> [5.3 XGBoost](#5.3-XGBoost)  \n",
    "> [5.4 Hyperparameter optimization](#5.4-Hyperparameter-optimization)  \n",
    "> [5.5 Pipeline](#5.5-Pipeline)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ml algos\n",
    "import xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Dask base\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Dask preprocessing\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "# Dask ml algos\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "import dask_xgboost\n",
    "\n",
    "# Dask pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*In order to leverage on the cloud solutions existent, I'm reading the file directly from a private owned S3 container in AWS.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_org = dd.read_csv( # Loading into a dask dataframe\n",
    "    'https://s3.eu-west-2.amazonaws.com/adv-py-assignment/hour.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Dask basic configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To ensure the most efficient handling of data and modeling, I'm initializing the dask client in the beginning of the script.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/distributed/bokeh/core.py:57: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn('\\n' + msg)\n",
      "tornado.application - ERROR - Multiple exceptions in yield list\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 883, in callback\n",
      "    result_list.append(f.result())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/deploy/local.py\", line 208, in _start_worker\n",
      "    yield w._start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 157, in _start\n",
      "    response = yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 352, in start\n",
      "    self.child_stop_q = mp_context.Queue()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 102, in Queue\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 41, in __init__\n",
      "    self._reader, self._writer = connection.Pipe(duplex=False)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 517, in Pipe\n",
      "    fd1, fd2 = os.pipe()\n",
      "OSError: [Errno 24] Too many open files\n",
      "tornado.application - ERROR - Multiple exceptions in yield list\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 883, in callback\n",
      "    result_list.append(f.result())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/deploy/local.py\", line 208, in _start_worker\n",
      "    yield w._start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 157, in _start\n",
      "    response = yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 351, in start\n",
      "    self.init_result_q = init_q = mp_context.Queue()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 102, in Queue\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 41, in __init__\n",
      "    self._reader, self._writer = connection.Pipe(duplex=False)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 517, in Pipe\n",
      "    fd1, fd2 = os.pipe()\n",
      "OSError: [Errno 24] Too many open files\n",
      "tornado.application - ERROR - Multiple exceptions in yield list\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 883, in callback\n",
      "    result_list.append(f.result())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/deploy/local.py\", line 208, in _start_worker\n",
      "    yield w._start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 143, in _start\n",
      "    listen_args=self.listen_args)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/core.py\", line 272, in listen\n",
      "    self.listener.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 396, in start\n",
      "    backlog=backlog)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/netutil.py\", line 134, in bind_sockets\n",
      "    sock = socket.socket(af, socktype, proto)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/socket.py\", line 151, in __init__\n",
      "OSError: [Errno 24] Too many open files\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-49-abb6cff63278>\", line 4, in <module>\n",
      "    client = Client(n_workers=4, threads_per_worker=1)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/client.py\", line 636, in __init__\n",
      "    self.start(timeout=timeout)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/client.py\", line 759, in start\n",
      "    sync(self.loop, self._start, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/utils.py\", line 277, in sync\n",
      "    six.reraise(*error[0])\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/six.py\", line 693, in reraise\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/utils.py\", line 262, in f\n",
      "    result[0] = yield future\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/client.py\", line 822, in _start\n",
      "    yield self.cluster\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/asyncio/tasks.py\", line 603, in _wrap_awaitable\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/deploy/local.py\", line 191, in _start\n",
      "    yield [self._start_worker(**self.worker_kwargs) for i in range(n_workers)]\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 883, in callback\n",
      "    result_list.append(f.result())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/deploy/local.py\", line 208, in _start_worker\n",
      "    yield w._start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 157, in _start\n",
      "    response = yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 370, in start\n",
      "    yield self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/process.py\", line 35, in _call_and_set_future\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/process.py\", line 184, in _start\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 291, in _Popen\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_forkserver.py\", line 35, in __init__\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_forkserver.py\", line 51, in _launch\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/forkserver.py\", line 69, in connect_to_new_process\n",
      "OSError: [Errno 24] Too many open files\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2018, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/inspect.py\", line 1500, in getinnerframes\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/inspect.py\", line 1458, in getframeinfo\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/inspect.py\", line 725, in getmodule\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/posixpath.py\", line 376, in abspath\n",
      "OSError: [Errno 24] Too many open files\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# local connection to client\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "\n",
    "# If using a cluster, we should instead use the following code:\n",
    "# client = Client('ip:port')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In data preparation I just did some simple data transformation like removing some columns and doing some basic transformation using specific dask_ml preprocessing algo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data visualization**  \n",
    "In order to better gasp the information we have to modelling, we need to trace a correlations plot.\n",
    "This is as well an opportunity to show that a dask dataframe can be ploted as easy as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlations of the different variables.\n",
    "corrMatt = ddf[\n",
    "    [\n",
    "        \"temp\",\n",
    "        \"atemp\",\n",
    "        \"hum\",\n",
    "        \"windspeed\",\n",
    "        \"workingday\",\n",
    "        \"weekday\",\n",
    "        \"season\",\n",
    "        \"holiday\",\n",
    "        \"hr\",\n",
    "        \"cnt\",\n",
    "    ]\n",
    "].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 10)\n",
    "sns.heatmap(corrMatt, mask=mask, vmax=0.8, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data will show important insights for the modelling phase.\n",
    "First of all we will take a look at the variables that showed the highest correlation with the cnt values in the correlation matrix: temp and hum. Showing a clearly linear relation between these two variable and the count variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of the rentals on a daily basis there are clearly periods of higher demand during a day, ie the moments people have to get home from/ go to work/school, and the other moments of the day.\n",
    "Of course there is a big seasonal effect as well as an effect of wheteher it is a working day or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing columns**  \n",
    "As we are predicting **<font color=blue>'cnt'</font>** we will take out some columns that show high correlation and would bring high bias to the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cnt so casual and registered can be left out\n",
    "ddf = ddf_org.drop([\"casual\", \"registered\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove atemp because of the almost 1 to 1 correlation with temp.\n",
    "ddf = ddf.drop([\"atemp\", \"dteday\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy encoding**  \n",
    "Here we converted every column type that was an **<font color=grey>'object'</font>**, into categories in order to facilitate the dummy creation. In this particular case, this *\"dummyfying\"* does not produce any result because all the data is alread numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Categorizer()\n",
    "ddf = cat.fit_transform(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = DummyEncoder()\n",
    "ddf = dum.fit_transform(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf.dtypes\n",
    "# ddf1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train/test split**  \n",
    "And finally we end up with a train/ test split for the modelling fase.\n",
    "This split is based on the predicting goal - *predict **<font color=blue>'cnt'</font>** values for October onwards*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ddf[ddf[\"instant\"]<15212]\n",
    "test = ddf[ddf[\"instant\"]>=15212]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data ready for dask algos, we need to produce dask array to make it more compliant with the different algos put in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['instant', 'cnt'], axis=1).to_dask_array()\n",
    "y_train = train['cnt'].to_dask_array()\n",
    "\n",
    "x_test = test.drop(['instant', 'cnt'], axis=1).to_dask_array()\n",
    "y_test = test['cnt'].to_dask_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, in order to facilitate the pipeline creation fase, we are gonna do a fork of the dataset at this moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pp = train.drop(['instant', 'cnt'], axis=1)\n",
    "y_train_pp = train['cnt']\n",
    "\n",
    "x_test_pp = test.drop(['instant', 'cnt'], axis=1)\n",
    "y_test_pp = test['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Dask Machine Learning algos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Linear regression\n",
    "**With dask ml models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use as a baseline, we are going to build a basic Linear Regression. For this linear regression we are using a ***dask_ml algo***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_lr = LinearRegression()\n",
    "dask_lr.fit(x_train, y_train) # fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr = dask_lr.predict(x_test) # predicting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the r2 score we used a sklearn metric to show the smooth transition between dask and sklearn api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred=y_lr.compute(), y_true=y_test.compute()) # we use .compute() to convert values from the dask array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random forest\n",
    "**With dask api for sklearn - joblib parallel backend**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joblib is used as an api interface for sklearn from dask. This allows to run sklearn models with a dask dataframe and distributed processing. If we wanted to scale this model, it would be easier because of the use of this api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend(\"dask\"): # everything that runs inside this model will be used in dask parallel processing\n",
    "    rf_model = RandomForestRegressor(n_estimators=200)\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    y_rf = rf_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred=y_rf, y_true=y_test.compute()) # we use .compute() to convert values from the dask array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 XGBoost\n",
    "**With dask ml models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is probably the model with higher potential from a dask integration. Using a native dask ml algo I implemented the xgboost model with considerably higher performance when comparing with the normal sklearn algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": 5, \"alpha\": 10, \"n_estimators\": 5000, \"colsample_bytree\": 0.3}\n",
    "\n",
    "xgb_model = dask_xgboost.train(\n",
    "    client=client, data=x_train, labels=y_train, num_boost_round=10, params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb = dask_xgboost.predict(client, xgb_model, x_test).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_pred=y_xgb.compute(), y_true=y_test.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Hyperparameter optimization\n",
    "**Optimizing the best model - random forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a native dask algo to optimize the best model, in this case random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_searchcv import GridSearchCV\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 150, 200, 250, 300],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "search = GridSearchCV(rf_model, params, cv=3)\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "best_grid = search.best_estimator_\n",
    "y_rf_o = best_grid.predict(x_test)\n",
    "\n",
    "r2_score(y_pred=y_rf_o, y_true=y_test.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Pipeline\n",
    "**Wrapping everything in a pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a pipeline to show how it could work with dask algos. In this pipeline i included some preprocessing transformations as well as the best model I encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process 48170 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 47554 was killed by unknown signal--- Logging error ---\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<function wrap.<locals>.null_wrapper at 0x1c69fb4d90>, 255)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 97, in _event_pipe\n",
      "    event_pipe = self._local.event_pipe\n",
      "AttributeError: '_thread._local' object has no attribute 'event_pipe'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1036, in emit\n",
      "    stream.write(msg)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 400, in write\n",
      "    self.pub_thread.schedule(lambda : self._buffer.write(string))\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 203, in schedule\n",
      "    self._event_pipe.send(b'')\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 101, in _event_pipe\n",
      "    event_pipe = ctx.socket(zmq.PUSH)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/zmq/sugar/context.py\", line 146, in socket\n",
      "    s = self._socket_class(self, socket_type, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\", line 59, in __init__\n",
      "    super(Socket, self).__init__(*a, **kw)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 328, in zmq.backend.cython.socket.Socket.__init__\n",
      "zmq.error.ZMQError: Too many open files\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 97, in _event_pipe\n",
      "    event_pipe = self._local.event_pipe\n",
      "AttributeError: '_thread._local' object has no attribute 'event_pipe'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/process.py\", line 115, in _on_exit\n",
      "    self._exit_callback(self)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 388, in _on_exit\n",
      "    self.mark_stopped()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 414, in mark_stopped\n",
      "    logger.warning(msg)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1395, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1519, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1529, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1591, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 905, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 1040, in emit\n",
      "    self.handleError(record)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/logging/__init__.py\", line 957, in handleError\n",
      "    sys.stderr.write('--- Logging error ---\\n')\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 400, in write\n",
      "    self.pub_thread.schedule(lambda : self._buffer.write(string))\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 203, in schedule\n",
      "    self._event_pipe.send(b'')\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel/iostream.py\", line 101, in _event_pipe\n",
      "    event_pipe = ctx.socket(zmq.PUSH)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/zmq/sugar/context.py\", line 146, in socket\n",
      "    s = self._socket_class(self, socket_type, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\", line 59, in __init__\n",
      "    super(Socket, self).__init__(*a, **kw)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 328, in zmq.backend.cython.socket.Socket.__init__\n",
      "zmq.error.ZMQError: Too many open files\n",
      "distributed.nanny - WARNING - Worker process 48168 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 47552 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 48169 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 47553 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 48167 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Worker process 47551 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - ERROR - Failed to restart worker after its process exited\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 291, in _on_exit\n",
      "    yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 370, in start\n",
      "    yield self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/process.py\", line 35, in _call_and_set_future\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/process.py\", line 184, in _start\n",
      "    process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 291, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_forkserver.py\", line 35, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/popen_forkserver.py\", line 47, in _launch\n",
      "    reduction.dump(process_obj, buf)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 957, in reduce_connection\n",
      "    df = reduction.DupFd(conn.fileno())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 170, in fileno\n",
      "    self._check_closed()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 136, in _check_closed\n",
      "    raise OSError(\"handle is closed\")\n",
      "OSError: handle is closed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - ERROR - Failed to restart worker after its process exited\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 291, in _on_exit\n",
      "    yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 351, in start\n",
      "    self.init_result_q = init_q = mp_context.Queue()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 102, in Queue\n",
      "    return Queue(maxsize, ctx=self.get_context())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 42, in __init__\n",
      "    self._rlock = ctx.Lock()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 67, in Lock\n",
      "    return Lock(ctx=self.get_context())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 162, in __init__\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 59, in __init__\n",
      "OSError: [Errno 24] Too many open files\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - ERROR - Failed to restart worker after its process exited\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 291, in _on_exit\n",
      "    yield self.instantiate()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1141, in run\n",
      "    yielded = self.gen.throw(*exc_info)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 226, in instantiate\n",
      "    self.process.start()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 1133, in run\n",
      "    value = future.result()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/tornado/gen.py\", line 326, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/site-packages/distributed/nanny.py\", line 351, in start\n",
      "    self.init_result_q = init_q = mp_context.Queue()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 102, in Queue\n",
      "    return Queue(maxsize, ctx=self.get_context())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 42, in __init__\n",
      "    self._rlock = ctx.Lock()\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/context.py\", line 67, in Lock\n",
      "    return Lock(ctx=self.get_context())\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 162, in __init__\n",
      "  File \"/usr/local/anaconda3/lib/python3.7/multiprocessing/synchronize.py\", line 59, in __init__\n",
      "OSError: [Errno 24] Too many open files\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    Categorizer(),\n",
    "    DummyEncoder(),\n",
    "    RandomForestRegressor(n_estimators=200))\n",
    "\n",
    "y_rf_pipe = pipe.fit(x_train_pp, y_train_pp).predict(x_test_pp)\n",
    "\n",
    "r2_score(y_pred=y_rf_pipe, y_true=y_test.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
